==================== 
FILE: server_package/config.py 

from configparser import ConfigParser


def load_config(filename='settings.ini', section=None):
    if section is None:
        raise ValueError("Section must be specified")

    parser = ConfigParser()
    parser.read(filename)

    if not parser.has_section(section):
        raise Exception(f'Section {section} not found in the {filename} file')

    return {param[0]: param[1] for param in parser.items(section)}


def db_config(filename='settings.ini'):
    return load_config(filename, 'postgresql')


def connection_pool_config(filename='settings.ini'):
    return load_config(filename, 'connection_pool')


def server_data(filename='settings.ini'):
    return load_config(filename, 'server_data')


def stress_test(filename='settings.ini'):
    return load_config(filename, 'stress_test')


def test_connection_usage(filename='settings.ini'):
    return load_config(filename, 'test_connection_usage') 

==================== 
FILE: server_package/conn_pool.py 

import time
import threading
from psycopg2 import connect as pg_connect
from connection_pool.server_package.config import db_config
from connection_pool.server_package.logger_config import setup_logger

logger = setup_logger('connection_pool_logger')


class ConnectionFactory:
    @staticmethod
    def create_new_connection():
        params = db_config()
        conn = pg_connect(**params)
        logger.debug(f"New connection created: {conn}")
        return conn


class ConnectionManager:
    def __init__(self, minconn, maxconn):
        self.minconn = int(minconn)
        self.maxconn = int(maxconn)
        self.all_connections = []
        self.in_use_conn = 0
        self.threads_waiting = 0
        self.total_threads = 0
        self.lock = threading.Lock()
        self.semaphore = threading.BoundedSemaphore(self.maxconn)
        self._initialize_pool()

    def _initialize_pool(self):
        with self.lock:
            for _ in range(self.minconn):
                self.all_connections.append(ConnectionFactory.create_new_connection())
            logger.info(f"Initialized connection pool with {self.minconn} connections.")

    def acquire(self):
        logger.debug("Attempting to acquire semaphore...")
        self._increment_thread_waiting()

        if not self.semaphore.acquire(timeout=10):
            self._decrement_thread_waiting()
            logger.error("Failed to acquire a connection: Timeout")
            raise Exception("Failed to acquire a connection: Timeout")

        conn = self._get_connection()

        self._decrement_thread_waiting()
        return conn

    def release(self, conn):
        if not conn or self.is_connection_closed(conn):
            logger.warning("Connection is already closed and will not be released.")
            return

        with self.lock:
            self.in_use_conn -= 1
            if len(self.all_connections) < self.maxconn:
                self.all_connections.append(conn)
                logger.info(f"Connection added back to pool: {conn}")
            else:
                self._close_connection(conn)
                logger.info(f"Connection closed as pool is full: {conn}")

        self.semaphore.release()

    def handle_connection_error(self, conn):
        with self.lock:
            if conn and not self.is_connection_closed(conn):
                self.in_use_conn -= 1
                self._close_connection(conn)
                logger.error(f"Connection closed due to error: {conn}")

            if len(self.all_connections) < self.maxconn:
                new_conn = ConnectionFactory.create_new_connection()
                self.all_connections.append(new_conn)
                logger.info(f"Replaced bad connection with a new one: {new_conn}")

        self.semaphore.release()

    def _get_connection(self):
        with self.lock:
            self.in_use_conn += 1
            if self.all_connections:
                conn = self.all_connections.pop()
                logger.info(f"Connection acquired from pool: {conn}")
            else:
                conn = ConnectionFactory.create_new_connection()
                logger.info(f"New connection created and acquired: {conn}")
            return conn

    def _increment_thread_waiting(self):
        with self.lock:
            self.threads_waiting += 1
            self.total_threads += 1

    def _decrement_thread_waiting(self):
        with self.lock:
            self.threads_waiting -= 1

    def is_connection_closed(self, conn):
        return conn.closed

    def _close_connection(self, conn):
        try:
            if not self.is_connection_closed(conn):
                conn.close()
                logger.debug(f"Connection closed: {conn}")
            else:
                logger.debug(f"Connection was already closed: {conn}")
        except Exception as e:
            logger.error(f"Error closing connection: {e}")

    def info(self):
        with self.lock:
            total_connections = self.in_use_conn + len(self.all_connections)
            logger.info(
                f"In use: {self.in_use_conn}, Available: {len(self.all_connections)}, Total: {total_connections}, "
                f"Threads waiting: {self.threads_waiting}, Total threads: {self.total_threads}")

    def maintain_minconn(self):
        with self.lock:
            while len(self.all_connections) < self.minconn:
                self.all_connections.append(ConnectionFactory.create_new_connection())
            logger.debug(f"Ensured minimum connections. Available: {len(self.all_connections)}")


class ConnectionCleanupTask:
    def __init__(self, manager: ConnectionManager, cleanup_interval):
        self.manager = manager
        self.cleanup_interval = int(cleanup_interval)
        self.start_cleanup_thread()

    def start_cleanup_thread(self):
        cleanup_thread = threading.Thread(target=self._cleanup_task, daemon=True)
        cleanup_thread.start()

    def _cleanup_task(self):
        while True:
            time.sleep(self.cleanup_interval)
            self._cleanup_pool()

    def _cleanup_pool(self):
        with self.manager.lock:
            logger.info(f"Starting cleanup. Available connections before cleanup: {len(self.manager.all_connections)}")
            while len(self.manager.all_connections) > self.manager.minconn:
                conn = self.manager.all_connections.pop()
                self.manager._close_connection(conn)
            logger.info(f"Cleanup finished. Available connections after cleanup: {len(self.manager.all_connections)}")
 

==================== 
FILE: server_package/connect.py 

from connection_pool.server_package.conn_pool import ConnectionManager, ConnectionCleanupTask
from connection_pool.server_package.config import connection_pool_config
from connection_pool.server_package.logger_config import setup_logger

logger = setup_logger('connect_logger')


class DatabaseConnectionError(Exception):
    pass


params = connection_pool_config()
manager = ConnectionManager(params['minconn'], params['maxconn'])
cleanup_task = ConnectionCleanupTask(manager, params['cleanup_interval'])


def connect():
    try:
        return manager.acquire()
    except Exception as e:
        logger.error(f"[CONNECT ERROR] Failed to acquire connection: {e}")
        raise DatabaseConnectionError(f"Connect error = {e}")


def release_connection(conn):
    try:
        manager.release(conn)
    except Exception as e:
        logger.error(f"[RELEASE ERROR] Failed to release connection: {e}")
        raise e


def handle_connection_error(conn):
    try:
        manager.handle_connection_error(conn)
    except Exception as e:
        logger.error(f"[HANDLE ERROR] Failed to handle connection error: {e}")
        raise e


def info():
    manager.info()

 

==================== 
FILE: server_package/logger_config.py 

import logging
import colorlog


def setup_logger(name='my_logger', level=logging.DEBUG):
    formatter = colorlog.ColoredFormatter(
        "%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt='%Y-%m-%d %H:%M:%S',
        log_colors={
            'DEBUG': 'cyan',
            'INFO': 'green',
            'WARNING': 'yellow',
            'ERROR': 'red',
            'CRITICAL': 'bold_red',
        }
    )
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.addHandler(handler)
    logger.setLevel(level)

    return logger 

==================== 
FILE: server_package/stress_test.py 

import time
import random
from concurrent.futures import ThreadPoolExecutor
from connection_pool.server_package.connect import connect, release_connection, handle_connection_error, info
from connection_pool.server_package.config import stress_test
from psycopg2 import sql
from connection_pool.server_package.logger_config import setup_logger

logger = setup_logger('stress_test_logger')
params = stress_test()
NUM_THREADS = int(params['num_threads'])
TEST_DURATION = int(params['test_duration'])


def stress_test_operation(thread_id):
    start_time = time.time()
    conn = None
    try:
        while time.time() - start_time < TEST_DURATION:
            try:
                conn = connect()
                with conn.cursor() as cur:
                    if random.random() < 0.1:
                        cur.execute("SELECT * FROM non_existing_table")
                        logger.warning(f"[SELECT ERROR] Thread {thread_id}: Tried to select from non-existing table.")
                    else:
                        operation = random.choice(["insert", "select"])
                        if operation == "insert":
                            query = sql.SQL("INSERT INTO items (item_name, item_quantity) VALUES (%s, %s)")
                            cur.execute(query, (f'Item {random.randint(1, 100000)}', random.randint(1, 100)))
                            logger.info(f"[INSERT] Thread {thread_id}: Inserted new item.")
                        elif operation == "select":
                            query = sql.SQL("SELECT * FROM items ORDER BY item_id DESC LIMIT 1")
                            cur.execute(query)
                            logger.info(f"[SELECT] Thread {thread_id}: Executed SELECT.")
                            result = cur.fetchone()
                            if result:
                                logger.debug(f"Thread {thread_id}: {result}")
                    conn.commit()
                info()
            except Exception as e:
                if conn:
                    handle_connection_error(conn)
                logger.error(f"Thread {thread_id} encountered an error: {e}")
            finally:
                if conn:
                    release_connection(conn)
                    conn = None
            time.sleep(random.uniform(0.01, 0.1))
    except Exception as e:
        logger.critical(f"Thread {thread_id} encountered a fatal error: {e}")


def run_stress_test():
    start_time = time.time()
    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        futures = [executor.submit(stress_test_operation, thread_id) for thread_id in range(NUM_THREADS)]
        for future in futures:
            future.result()
    logger.info(f"Stress test completed in {time.time() - start_time} seconds")


if __name__ == "__main__":
    run_stress_test()
    logger.info("End of Stress Test") 


====================
FILE: server_package/settings.ini

[postgresql]
host=127.0.0.1
database=CP_BASE
user=pozamiataj
password=pozamiataj.pl

[connection_pool]
minconn=5
maxconn=100
cleanup_interval=29

[server_data]
host=127.0.0.1
port=65432
buffer_size=1024

[stress_test]
num_threads=110
test_duration=120