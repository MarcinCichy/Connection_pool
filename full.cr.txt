==================== 
FILE: server_package/config.py 

from configparser import ConfigParser


def load_config(filename='settings.ini', section=None):
    if section is None:
        raise ValueError("Section must be specified")

    parser = ConfigParser()
    parser.read(filename)

    if not parser.has_section(section):
        raise Exception(f'Section {section} not found in the {filename} file')

    return {param[0]: param[1] for param in parser.items(section)}


def db_config(filename='settings.ini'):
    return load_config(filename, 'postgresql')


def connection_pool_config(filename='settings.ini'):
    return load_config(filename, 'connection_pool')


def server_data(filename='settings.ini'):
    return load_config(filename, 'server_data')


def stress_test(filename='settings.ini'):
    return load_config(filename, 'stress_test')


def test_connection_usage(filename='settings.ini'):
    return load_config(filename, 'test_connection_usage')

==================== 
FILE: server_package/conn_pool.py 

import time
from psycopg2 import connect as pg_connect
from connection_pool.server_package.config import db_config
from connection_pool.server_package.logger_config import setup_logger
import threading

logger = setup_logger('connection_pool_logger')


class ConnectionPool:
    def __init__(self, minconn, maxconn, cleanup_interval):
        self.minconn = int(minconn)
        self.maxconn = int(maxconn)
        self.cleanup_interval = int(cleanup_interval)
        self.all_connections = []
        self.in_use_conn = 0
        self.threads_waiting = 0
        self.total_threads = 0
        self.lock = threading.Lock()
        self.semaphore = threading.BoundedSemaphore(self.maxconn)
        self.initialize_pool()
        self.start_cleanup_thread()

    def initialize_pool(self):
        with self.lock:
            for _ in range(self.minconn):
                self.all_connections.append(self.create_new_connection())
            logger.info(f"Initialized connection pool with {self.minconn} connections.")

    def create_new_connection(self):
        params = db_config()
        conn = pg_connect(**params)
        logger.debug(f"New connection created: {conn}")
        return conn

    def acquire(self):
        logger.debug("Attempting to acquire semaphore...")
        with self.lock:
            self.threads_waiting += 1
            self.total_threads += 1
        if not self.semaphore.acquire(timeout=10):
            with self.lock:
                self.threads_waiting -= 1
            logger.error("Failed to acquire a connection: Timeout")
            raise Exception("Failed to acquire a connection: Timeout")

        with self.lock:
            self.threads_waiting -= 1
            if self.all_connections:
                conn = self.all_connections.pop()
                logger.info(f"Connection acquired from pool: {conn}")
            else:
                conn = self.create_new_connection()
                logger.info(f"New connection created and acquired: {conn}")
            self.in_use_conn += 1
            return conn

    def release(self, conn):
        with self.lock:
            if not conn:
                logger.warning("No connection provided to release.")
                return
            if self.is_connection_closed(conn):
                logger.warning(f"Connection is already closed and will not be released: {conn}")
                return

            self.in_use_conn -= 1
            if not self.is_connection_closed(conn):
                if len(self.all_connections) < self.maxconn:
                    self.all_connections.append(conn)
                    logger.info(f"Connection added back to pool: {conn}")
                else:
                    self.close_connection(conn)
                    logger.info(f"Connection closed as pool is full: {conn}")
            else:
                logger.info(f"Connection was already closed and not added to pool: {conn}")

            self.semaphore.release()

    def handle_connection_error(self, conn):
        logger.error(f"Handling connection error for: {conn}")
        with self.lock:
            if conn and not self.is_connection_closed(conn):
                self.in_use_conn -= 1
                self.close_connection(conn)
                logger.error(f"Connection closed due to error: {conn}")

            if len(self.all_connections) < self.maxconn:
                new_conn = self.create_new_connection()
                self.all_connections.append(new_conn)
                logger.info(f"Replaced bad connection with a new one: {new_conn}")

            self.semaphore.release()

    def cleanup_pool(self):
        with self.lock:
            logger.info(f"Starting cleanup. Available connections before cleanup: {len(self.all_connections)}")
            while len(self.all_connections) > self.minconn:
                conn = self.all_connections.pop()
                self.close_connection(conn)
            logger.info(f"Cleanup finished. Available connections after cleanup: {len(self.all_connections)}")

    def close_connection(self, conn):
        try:
            if not self.is_connection_closed(conn):
                conn.close()
                logger.debug(f"Connection closed: {conn}")
            else:
                logger.debug(f"Connection was already closed: {conn}")
        except Exception as e:
            logger.error(f"Error closing connection: {e}")

    def is_connection_closed(self, conn):
        return conn.closed

    def info(self):
        with self.lock:
            total_connections = self.in_use_conn + len(self.all_connections)
            logger.info(f"In use: {self.in_use_conn}, Available: {len(self.all_connections)}, Total: {total_connections}, "
                         f"Threads waiting: {self.threads_waiting}, Total threads: {self.total_threads}")

    def maintain_minconn(self):
        """Ensure there are at least minconn connections in the pool."""
        with self.lock:
            while len(self.all_connections) < self.minconn:
                self.all_connections.append(self.create_new_connection())
            logger.debug(f"Ensured minimum connections. Available: {len(self.all_connections)}")

    def start_cleanup_thread(self):
        """Start a background thread that cleans up the pool periodically."""
        cleanup_thread = threading.Thread(target=self.cleanup_task, daemon=True)
        cleanup_thread.start()

    def cleanup_task(self):
        """Periodically clean up the pool to ensure it's not overpopulated."""
        while True:
            time.sleep(self.cleanup_interval)
            self.cleanup_pool_async()

    def cleanup_pool_async(self):
        """Perform cleanup asynchronously."""
        threading.Thread(target=self.cleanup_pool).start()

==================== 
FILE: server_package/connect.py 

from connection_pool.server_package.conn_pool import ConnectionPool
from connection_pool.server_package.config import connection_pool_config
from connection_pool.server_package.logger_config import setup_logger

logger = setup_logger('connect_logger')


class DatabaseConnectionError(Exception):
    pass


params = connection_pool_config()
pool = ConnectionPool(params['minconn'], params['maxconn'], params['cleanup_interval'])


def connect():
    try:
        return pool.acquire()
    except Exception as e:
        logger.error(f"[CONNECT ERROR] Failed to acquire connection: {e}")
        raise DatabaseConnectionError(f"Connect error = {e}")


def release_connection(conn):
    try:
        pool.release(conn)
    except Exception as e:
        logger.error(f"[RELEASE ERROR] Failed to release connection: {e}")
        raise e


def handle_connection_error(conn):
    try:
        pool.handle_connection_error(conn)
    except Exception as e:
        logger.error(f"[HANDLE ERROR] Failed to handle connection error: {e}")
        raise e


def info():
    pool.info()

 

==================== 
FILE: server_package/stress_test.py 

import time
import random
from concurrent.futures import ThreadPoolExecutor
from connection_pool.server_package.connect import connect, release_connection, handle_connection_error, info
from connection_pool.server_package.config import stress_test
from psycopg2 import sql
from connection_pool.server_package.logger_config import setup_logger

logger = setup_logger('stress_test_logger')
params = stress_test()
NUM_THREADS = int(params['num_threads'])
TEST_DURATION = int(params['test_duration'])


def stress_test_operation(thread_id):
    start_time = time.time()
    conn = None
    try:
        while time.time() - start_time < TEST_DURATION:
            try:
                conn = connect()
                with conn.cursor() as cur:
                    if random.random() < 0.1:
                        cur.execute("SELECT * FROM non_existing_table")
                        logger.warning(f"[SELECT ERROR] Thread {thread_id}: Tried to select from non-existing table.")
                    else:
                        operation = random.choice(["insert", "select"])
                        if operation == "insert":
                            query = sql.SQL("INSERT INTO items (item_name, item_quantity) VALUES (%s, %s)")
                            cur.execute(query, (f'Item {random.randint(1, 100000)}', random.randint(1, 100)))
                            logger.info(f"[INSERT] Thread {thread_id}: Inserted new item.")
                        elif operation == "select":
                            query = sql.SQL("SELECT * FROM items ORDER BY item_id DESC LIMIT 1")
                            cur.execute(query)
                            logger.info(f"[SELECT] Thread {thread_id}: Executed SELECT.")
                            result = cur.fetchone()
                            if result:
                                logger.debug(f"Thread {thread_id}: {result}")
                    conn.commit()
                info()
            except Exception as e:
                if conn:
                    handle_connection_error(conn)
                logger.error(f"Thread {thread_id} encountered an error: {e}")
            finally:
                if conn:
                    release_connection(conn)
                    conn = None
            time.sleep(random.uniform(0.01, 0.1))
    except Exception as e:
        logger.critical(f"Thread {thread_id} encountered a fatal error: {e}")


def run_stress_test():
    start_time = time.time()
    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        futures = [executor.submit(stress_test_operation, thread_id) for thread_id in range(NUM_THREADS)]
        for future in futures:
            future.result()
    logger.info(f"Stress test completed in {time.time() - start_time} seconds")


if __name__ == "__main__":
    run_stress_test()
    logger.info("End of Stress Test")
 

====================
FILE: server_package/logger_config.py

import logging
import colorlog


def setup_logger(name='my_logger', level=logging.DEBUG):
    formatter = colorlog.ColoredFormatter(
        "%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt='%Y-%m-%d %H:%M:%S',
        log_colors={
            'DEBUG': 'cyan',
            'INFO': 'green',
            'WARNING': 'yellow',
            'ERROR': 'red',
            'CRITICAL': 'bold_red',
        }
    )
    handler = logging.StreamHandler()
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.addHandler(handler)
    logger.setLevel(level)

    return logger

====================
FILE: server_package/settings.ini 

[postgresql]
host=127.0.0.1
database=CP_BASE
user=pozamiataj
password=pozamiataj.pl

[connection_pool]
minconn=5
maxconn=100
cleanup_interval=29

[server_data]
host=127.0.0.1
port=65432
buffer_size=1024

[stress_test]
num_threads=110
test_duration=120